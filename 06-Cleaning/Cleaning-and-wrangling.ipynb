{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fbff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c163bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e1b51",
   "metadata": {},
   "source": [
    "## Common Data Cleaning Tasks\n",
    "\n",
    "* Handling missing data\n",
    "* Handling duplicates\n",
    "* Detecting and filtering outliers\n",
    "* Discretize and bin\n",
    "* Creating dummy variables, combine categories, other encodings\n",
    "* Changing data types\n",
    "* Unpacking (extract info from a column)\n",
    "* Tidying data\n",
    "\n",
    "*Note:*  Regular expressions and string manipulation methods are often useful in data cleaning, particularly when there is some form of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddcb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[1., 6.5, 3.0, 0.5], [1., np.nan, 3, 1.5],\n",
    "                     [np.nan, np.nan, np.nan, np.nan], [np.nan, 6.5, np.nan, 3.5],\n",
    "                     [np.nan, 6.5, 3.0, 3.5]],\n",
    "                 columns=['A','B','C','D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00734e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d051fc8",
   "metadata": {},
   "source": [
    "## Missing data\n",
    "\n",
    "Data that is to be used in a machine or statistical learning model needs to be absent of missing values.  Below are some ways to handle missing values in pandas.  The \"correct\" way to treat missing values will be context specific.  Analyses might compare results from how missing values were handled differently.  Often, missing values are filled using `Scikit-Learn` so the process can be built into a pipeline. \n",
    "\n",
    "1. **Detecting Missing Values**:\n",
    "    - `isna()` or `isnull()`: Returns a DataFrame or Series indicating which values are missing.\n",
    "    - `notna()` or `notnull()`: Opposite of `isna()`, indicates which values are not missing.\n",
    "  \n",
    "2. **Removing Missing Values**:\n",
    "    - `dropna()`: \n",
    "        - Remove missing values.\n",
    "        - Can specify `axis` (rows or columns).\n",
    "        - Can use `how` argument to specify 'any' (default) or 'all' to determine when rows/columns are dropped.\n",
    "        - Can use `subset` argument to specify to only consider certain  columns or rows\n",
    "\n",
    "3. **Filling Missing Values**:\n",
    "    - `fillna()`:\n",
    "        - Fill missing values with a specific value.\n",
    "        - Use `method` argument to specify method ('ffill' for forward fill, 'bfill' for backward fill).\n",
    "        - Can fill using the mean, median, or mode.\n",
    "        - Can also use interpolation methods.\n",
    "\n",
    "4. **Replace Missing Values**:\n",
    "    - `replace()`: \n",
    "        - Replace specified values with another value (can be used for values other than NaN too).\n",
    "\n",
    "5. **Using an Indicator**:\n",
    "    - Add a new column to indicate where values are missing: \n",
    "        ```python\n",
    "        df['missing_col_indicator'] = df['col'].isnull()\n",
    "        ```\n",
    "\n",
    "6. **Group-wise Handling**:\n",
    "    - Use `groupby` along with `fillna()` to fill missing values based on some group properties:\n",
    "        ```python\n",
    "        df['col'] = df.groupby('group_col')['col'].transform(lambda x: x.fillna(x.mean()))\n",
    "        ```\n",
    "\n",
    "7. **Forward and Backward Filling**:\n",
    "    - `ffill()` or `pad()`: Fill values forward.\n",
    "    - `bfill()` or `backfill()`: Fill values backward.\n",
    "\n",
    "8. **Limiting Fill or Drop Actions**:\n",
    "    - Using the `limit` parameter in methods like `dropna()` and `fillna()` to restrict the **number** of rows/columns affected.\n",
    "    - Using the `subset` argument in methods like `dropna()` and `fillna()` to specify the actual row/columns affected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6af2e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1a3aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed16b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c5dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['D', 'C'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(value=df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279a5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3e19b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Duplicates\n",
    "\n",
    "Handling duplicate data is important, especially in data preprocessing, to ensure the accuracy of subsequent analyses. Here's a bullet list of possible ways to check for and handle duplicate data in pandas:\n",
    "\n",
    "1. **Checking for Duplicates**:\n",
    "    - `duplicated()`: Returns a Boolean series that indicates whether each row is a duplicate or not.\n",
    "\n",
    "        ```python\n",
    "        df = pd.DataFrame({'A': [1, 2, 2, 3, 3], 'B': [1, 1, 2, 2, 3]})\n",
    "        print(df.duplicated())\n",
    "        ```\n",
    "\n",
    "2. **Counting Duplicates**:\n",
    "    - Chain the `sum()` function after `duplicated()` to get the total count of duplicate rows.\n",
    "\n",
    "        ```python\n",
    "        print(df.duplicated().sum())\n",
    "        ```\n",
    "\n",
    "3. **Viewing Duplicates**:\n",
    "    - Use boolean indexing to filter and see the actual duplicate rows.\n",
    "\n",
    "        ```python\n",
    "        print(df[df.duplicated()])\n",
    "        ```\n",
    "\n",
    "4. **Removing Duplicates**:\n",
    "    - `drop_duplicates()`: Removes the duplicate rows from the DataFrame.\n",
    "\n",
    "        ```python\n",
    "        df_no_duplicates = df.drop_duplicates()\n",
    "        ```\n",
    "\n",
    "5. **Considering Specific Columns**:\n",
    "    - `drop_duplicates(subset=column_name)`: Check for duplicates based on specific columns.\n",
    "\n",
    "        ```python\n",
    "        df_no_dup_in_A = df.drop_duplicates(subset='A')\n",
    "        ```\n",
    "\n",
    "6. **Keeping Specific Duplicates**:\n",
    "    - The `keep` parameter in `drop_duplicates()`:\n",
    "        - `keep='first'` (default): Keeps the first occurrence of the duplicate.\n",
    "        - `keep='last'`: Keeps the last occurrence of the duplicate.\n",
    "        - `keep=False`: Removes all occurrences of the duplicates.\n",
    "\n",
    "        ```python\n",
    "        df_removed_all = df.drop_duplicates(keep=False)\n",
    "        ```\n",
    "\n",
    "7. **Checking Duplicates in Columns**:\n",
    "    - Sometimes you might want to check duplicates within a column.\n",
    "\n",
    "        ```python\n",
    "        print(df['A'].duplicated().sum())\n",
    "        ```\n",
    "\n",
    "8. **Removing Duplicates in Columns**:\n",
    "    - To drop duplicate values in a specific column, you can use `drop_duplicates` with the `subset` argument.\n",
    "\n",
    "        ```python\n",
    "        df_no_dup_column = df.drop_duplicates(subset='A', keep='first')\n",
    "        ```\n",
    "\n",
    "9. **Resetting Index**:\n",
    "    - After dropping duplicates, you might have gaps in your index. You can reset the index using `reset_index(drop=True)`.\n",
    "\n",
    "        ```python\n",
    "        df_reset = df_no_duplicates.reset_index(drop=True)\n",
    "        ```\n",
    "\n",
    "10. **Considering Data Order**:\n",
    "    - When handling duplicates, the order of data can matter. Ensure you've sorted the data (if necessary) before addressing duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac8a8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f462a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38391c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['B','D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764f3e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3845f41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c44b7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5b8005a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Outliers\n",
    "\n",
    "Handling outliers is essential as they can distort results and lead to incorrect conclusions. However, it is important to remember that not all outliers are bad or erroneous. Like many data cleaning steps, the appropriate way to handle outliers can depend on the problem.\n",
    "\n",
    "Here are a few possible ways to check for and handle outliers in pandas:\n",
    "\n",
    "\n",
    "\n",
    "1. **Visualization**:\n",
    "    - Using plots can help in visually identifying outliers.\n",
    "    - `boxplot()`: A box plot can reveal outliers as points beyond the \"whiskers\" of the box plot.\n",
    "\n",
    "        ```python\n",
    "        import matplotlib.pyplot as plt\n",
    "        df = pd.DataFrame({'Values': [1, 2, 3, 50]})\n",
    "        df.boxplot(column='Values')\n",
    "        plt.show()\n",
    "        ```\n",
    "\n",
    "2. **Descriptive Statistics**:\n",
    "    - `describe()`: Gives a summary of statistics, where the minimum and maximum values can provide insights about potential outliers.\n",
    "    \n",
    "        ```python\n",
    "        print(df.describe())\n",
    "        ```\n",
    "\n",
    "3. **Z-Score**:\n",
    "    - Measure the number of standard deviations a data point is from the mean.\n",
    "    - Data points with a z-score greater than 3 or less than -3 are typically considered outliers.\n",
    "        ```python\n",
    "        from scipy.stats import zscore\n",
    "        abs_z_scores = np.abs(zscore(df['Values']))\n",
    "        outliers = df[abs_z_scores > 3]\n",
    "        print(outliers)\n",
    "        ```\n",
    "\n",
    "4. **IQR (Interquartile Range)**:\n",
    "    - IQR is the range between the first quartile (25th percentile) and the third quartile (75th percentile).\n",
    "    - Data points outside 1.5 times the IQR below the first quartile or above the third quartile might be considered outliers.\n",
    "\n",
    "        ```python\n",
    "        Q1 = df['Values'].quantile(0.25)\n",
    "        Q3 = df['Values'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df[((df['Values'] < (Q1 - 1.5 * IQR)) | (df['Values'] > (Q3 + 1.5 * IQR)))]\n",
    "        print(outliers)\n",
    "        ```\n",
    "\n",
    "5. **Removing Outliers**:\n",
    "    - After identifying outliers, you can drop them using boolean indexing.\n",
    "\n",
    "        ```python\n",
    "        filtered_df = df[(abs_z_scores < 3)]\n",
    "        ```\n",
    "\n",
    "6. **Capping Outliers**:\n",
    "    - Instead of removing outliers, you can limit their value to a specific range (capping).\n",
    "\n",
    "        ```python\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df['Values'] = np.where(df['Values'] > upper_bound, upper_bound, df['Values'])\n",
    "        df['Values'] = np.where(df['Values'] < lower_bound, lower_bound, df['Values'])\n",
    "        ```\n",
    "\n",
    "7. **Transforming Data**:\n",
    "    - Outliers can sometimes be managed by transforming data to fit into a more regular shape.\n",
    "    - Common transformations include square root, log, and box-cox transformations.\n",
    "\n",
    "        ```python\n",
    "        df['Log_Values'] = np.log(df['Values'] + 1)\n",
    "        ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e8407",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "data = pd.DataFrame(np.random.standard_normal((10, 4)))\n",
    "data = data*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fadf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c64e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[data[0].abs() > 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f54b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data[(data.abs() > 4).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07837cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace outliers with .mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d276e",
   "metadata": {},
   "source": [
    "`.mask` is an application of the if-then idiom.  If \"condition\" is True, replace with \"value\".  There are also `inplace` and `axis` arguments.\n",
    "\n",
    "`df.mask(condition, value)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f246593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.quantile(.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fixed = data.mask(data>4, data.quantile(.9), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fixed = data_fixed.mask(data< -4, data.quantile(.1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21683484",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.mask(data.abs()>4, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a15593",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d20843",
   "metadata": {},
   "source": [
    "## Discretizing\n",
    "\n",
    "Another way to handle outliers, is to discretize or bin a continuous/numeric variable.  Other reasons for binning include (not an exhaustive list):\n",
    "- Data simplification\n",
    "- Visualization\n",
    "- Dealing with noise and variability\n",
    "- Improving model performance\n",
    "- Handling sparse data areas\n",
    "- Ensuring privacy\n",
    "- Etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab138c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df323fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [-10,-2,-1,1,2,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef14a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(s, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6136df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(s,bins).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84831015",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(s,bins).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2862f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.cut(s,bins,labels=['very low','low','average','high','very high'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f38876",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bed68a",
   "metadata": {},
   "source": [
    "## Dummy Variables\n",
    "\n",
    "Creating dummy variables is also known as One-hot encoding.  It is a possible method for converting categorical variables into numbers that can be fed into a machine or statistical learning model. \n",
    "\n",
    "\n",
    "1. **Binary Categorical Data**:\n",
    "    - For binary categorical data (e.g., \"Yes\" or \"No\"), you can represent it with one dummy variable (1 for \"Yes\" and 0 for \"No\").\n",
    "\n",
    "2. **Multi-category Categorical Data**:\n",
    "    - For a categorical variable with *n* distinct categories, you can represent it using *n-1* or *n* dummy variables, depending on the approach.\n",
    "    - Why *n-1* or *n*? Using *n-1* dummy variables avoids the \"dummy variable trap\" or multicollinearity, where one variable can be predicted from the others. However, in some situations, like with certain machine learning algorithms or when clarity is needed, all *n* dummy variables might be used.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let's take an example where a feature \"Color\" in a dataset has three categories: \"Red\", \"Green\", and \"Blue\".\n",
    "\n",
    "Using dummy variables, this can be represented as:\n",
    "\n",
    "| Color | Red | Green | Blue |\n",
    "|-------|-----|-------|------|\n",
    "| Red   | 1   | 0     | 0    |\n",
    "| Green | 0   | 1     | 0    |\n",
    "| Blue  | 0   | 0     | 1    |\n",
    "\n",
    "If you choose to use *n-1* dummy variables to avoid multicollinearity, you might drop one of the columns. If you drop the \"Blue\" column, the absence of \"Red\" and \"Green\" (i.e., both being 0) would imply \"Blue\".\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "- **Advantages**:\n",
    "    - Makes categorical data usable in models that require numerical input variables.\n",
    "    - Helps in revealing more granular patterns in the relationship between features and response variables.\n",
    "  \n",
    "- **Disadvantages**:\n",
    "    - Can lead to a substantial increase in dataset dimensions, especially with high-cardinality categorical variables.\n",
    "    - Can introduce multicollinearity if not handled correctly.\n",
    "\n",
    "- **Alternatives**:\n",
    "    - While dummy variables are a popular way to handle categorical data, there are other methods like ordinal encoding, binary encoding, and embedding layers (for deep learning models).\n",
    "\n",
    "In `pandas`, the `pd.get_dummies()` function can be used to easily convert categorical variables into dummy/indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame([['apple', 6.5, 3.0, 0.5], ['apple', np.nan, np.nan, 1.5],\n",
    "                     ['banana', np.nan, np.nan, np.nan], ['orange', 6.5, 3., 3.5],\n",
    "                     ['orange', 6.5, 3.0, 3.5]],\n",
    "                 columns=['A','B','C','D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bce0c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f6cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df2, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd1564e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c971461",
   "metadata": {},
   "source": [
    "---\n",
    "## Combining categories or other encodings\n",
    "\n",
    "Instead of just using dummy variables, sometimes we might want to use an ordinal or other type of encoding.  One way to do this would be to create a mapping dictionary.  \n",
    "\n",
    "```\n",
    "mapping_dict = {old_value:new_value, \n",
    "another_old:another_new}\n",
    "\n",
    "## apply\n",
    "df['category'].map(mapping_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a846c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'apple': 'Group1', 'banana': 'Group2', 'orange': 'Group1'}\n",
    "df2['New'] = df2['A'].map(mapping)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a01c51c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedd742b",
   "metadata": {},
   "source": [
    "## Changing data types (casting)\n",
    "\n",
    "Casting is the process of converting one data type into another.  Changing strings to numbers and numbers to strings is a common occurrence when cleaning data. Some pandas functions and methods used for casting include:\n",
    "\n",
    "\n",
    "1. **astype()**:\n",
    "    - The primary method to cast a pandas object to a specified data type.\n",
    "    ```python\n",
    "    df['column_name'] = df['column_name'].astype('new_data_type')\n",
    "    ```\n",
    "\n",
    "2. **to_numeric()**:\n",
    "    - Convert a Series or a single column of a DataFrame to a numeric data type (either integer or float).\n",
    "    - Useful for converting columns that might have been read as strings or objects due to the presence of non-numeric values.\n",
    "    ```python\n",
    "    df['column_name'] = pd.to_numeric(df['column_name'], errors='coerce')\n",
    "    ```\n",
    "\n",
    "3. **to_datetime()**:\n",
    "    - Convert a Series or column to a datetime data type.\n",
    "    ```python\n",
    "    df['date_column'] = pd.to_datetime(df['date_column'])\n",
    "    ```\n",
    "\n",
    "4. **to_timedelta()**:\n",
    "    - Convert a Series or column to a timedelta data type.\n",
    "    ```python\n",
    "    df['duration_column'] = pd.to_timedelta(df['duration_column'])\n",
    "    ```\n",
    "\n",
    "5. **pd.Categorical()**:\n",
    "    - Convert a Series or column to a categorical data type.\n",
    "    ```python\n",
    "    df['category_column'] = pd.Categorical(df['category_column'])\n",
    "    ```\n",
    "\n",
    "When casting variables, it's essential to be aware of potential data loss or errors. For instance, casting a floating-point column to an integer will truncate the decimal parts, and trying to convert non-numeric strings to integers or floats can raise errors. Always check the data after casting to ensure that the conversion has been done correctly and as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame([['apple', '6.5', 3.0, 0.5], ['apple', np.nan, np.nan, 1.5],\n",
    "                     ['banana', np.nan, np.nan, np.nan], ['orange', 6.5, 3., 3.5],\n",
    "                     ['orange', 6.5, 3.0, 3.5]],\n",
    "                 columns=['A','B','C','D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d95503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159adbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['B'] = df3['B'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add5397b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d53d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802f955",
   "metadata": {},
   "source": [
    "## Unpacking\n",
    "\n",
    "Many times, a single column will contain multiple pieces of information.  Learning how to extract or unpack this information is a common data cleaning task. \n",
    "\n",
    "Python's built in string methods and regular expressions are very helpful in these instances.  (See Python for Data Analysis Section 7.4)\n",
    "\n",
    "### Helpful built in string methods\n",
    "\n",
    "- `join`: Use string a delimiter for concatenating a sequence of other strings\n",
    "- `strip`: Trim whitespace - Can also remove characters in the `chars` argument\n",
    "- `split`: Split into a list of substrings based white space or `sep` argument\n",
    "- `lower`: Converts to lowercase\n",
    "- `replace`: Replace substring with another substring\n",
    "\n",
    "### Other functions\n",
    "\n",
    "Other functions, attributes, and methods that are commonly used in unpacking tasks include\n",
    "\n",
    "- `.apply`: Use to pally a function along the axis (either rows or columns) of a DataFrame or on the elements of a Series.\n",
    "- `lambda` functions\n",
    "- Accessor objects allow you to perform operations on a Series without the need of loops    \n",
    "    - `.str`: An accessor object used for vectorized string operations on a pandas Series\n",
    "    - `.dt`: An accessor object used for vectorized date operations on a pandas datetime Series\n",
    "    - *Further explanation:*   String and regular expression methods/functions  only work on `string` objects.  Date function/methods only work on `datetime` objects.  When dealing with pandas, the *value* inside a Series might be a `string` or `datetime`, but the Series itself is **not** a `string`.  Calling `.str` or .`dt` off of a Series allows `string` and `datetime` functions to be used on the Series.\n",
    "    ```python\n",
    "    s = pd.Series(['apple', 'banana', 'cherry'])    \n",
    "    s.str[:3]\n",
    "    s.str.startswith('a')\n",
    "\n",
    "    dates = pd.Series(pd.date_range(\"2022-01-01\", periods=3, freq=\"Y\"))\n",
    "    dates.dt.year\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72654d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffd63bab",
   "metadata": {},
   "source": [
    "---\n",
    "# Tidy Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b5ecfa",
   "metadata": {},
   "source": [
    "## Melting: wide to long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb09194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd2e3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8d884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pew = pd.read_csv('Data/pew-raw.csv')\n",
    "pew.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "pew.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "pd.melt(pew,id_vars='religion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abf544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method\n",
    "pew = pew.melt(id_vars='religion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdbb315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58da0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7310b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2575fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6708c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f967edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0aa79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93dae8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c974b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd56ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b9241d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8612ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = [' <$10k', ' $10-20k', '$20-30k', '$30-40k', ' $40-50k',\n",
    "       '$50-75k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374eb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.melt(pew, id_vars='religion',value_vars=varnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dccb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f2d36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = pd.read_csv('Data/billboard.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d3ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3a4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(bb, id_vars=['artist','track'],value_vars=bb.columns[7:],\n",
    "        var_name='week', value_name='rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83edd462",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(bb, id_vars=bb.columns[0:7], var_name='week', value_name='rank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ba0d53",
   "metadata": {},
   "source": [
    "## Pivoting:  long to wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed220539",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = pd.read_csv('Data/stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef1ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70b2904",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = st.pivot(index='date', columns='symbol', values=['open','close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b78f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afcabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f072c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5951f94d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4322825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.pivot(index='date',columns='symbol',values=['open','close']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beedd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "st = st.iloc[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726aa201",
   "metadata": {},
   "outputs": [],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_table = pd.pivot(st, index='date', columns='symbol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5640ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_table.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edadfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb38ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c99f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_new = pd.pivot(st, index='date', columns='symbol', values=['open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de26ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_new.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_new.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d80321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = pd.read_csv('Data/country_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44bb4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "country.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4e13ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.pivot(country, index=['Country','Region'], columns='Variable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(country, index=['Country','Region'], columns='Variable', values='Value',aggfunc='mean').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991bb024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eca1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c5da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cebf4ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c591f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9760e945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e48f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = pd.pivot_table(country,index=['Country','Region'], \n",
    "         columns='Variable', values='Value').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29399d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(country,index=['Country'], \n",
    "         columns=['Region','Variable'], values='Value').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e570c985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46350c28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f99dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classes24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
